#!/usr/bin/env python3

"""
Find identical files in the current directory (recursively) and change them
to (hard) links to the same inode. Useful if you're dealing with immutable
files that have been copied around several times and you want to reduce the
storage space they actually use.

Please make sure you understand the consequences of using hard links.

Install: just put this file in your PATH. Make sure it's executable.
The script will use https://pypi.org/project/tqdm/ if it's installed
but will work well without it.

Usage: `relink` will show you what `relink --force` would do.
"""

import hashlib
import multiprocessing
import os
import sys


try:
    from tqdm import tqdm
except ImportError:
    def tqdm(x, *args, desc, **kwds):
        print(f"{desc}...")
        return x


def main():
    file_paths = list(find_file_paths())

    print(f"Found {len(file_paths)} files", flush=True, end="")

    # @todo Parallelize calls to os.stat?
    ino_to_paths = dict_of_sets((os.stat(file_path).st_ino, file_path) for file_path in file_paths)

    print(f" pointing at {len(ino_to_paths)} inodes", flush=True)

    with multiprocessing.Pool() as p:
        hash_to_inos = dict_of_sets(tqdm(
            p.imap_unordered(hash_in_pool, ino_to_paths.items()),
            desc="Hashing",
            total=len(ino_to_paths),
        ))

    # @todo Parallelize calls to os.link?
    for (src, dst) in tqdm(list(compute_relinks(hash_to_inos, ino_to_paths)), desc="Relinking"):
        relink(src, dst)


def find_file_paths():
    for (dir_path, dir_names, file_names) in os.walk("."):
        for file_name in file_names:
            file_path = os.path.join(dir_path, file_name)
            if os.path.isfile(file_path):
                yield file_path


def dict_of_sets(items):
    d = {}
    for (k, v) in items:
        d.setdefault(k, set()).add(v)
    return d


def hash_in_pool(args):
    (ino, file_paths) = args
    return (hash_file(elem(file_paths)), ino)


def elem(s):
    return next(iter(s))


def hash_file(file_path):
    with open(file_path, "rb") as f:
        file_hash = hashlib.sha224()
        while True:
            chunk = f.read(2 ** 20)
            if not chunk:
                break
            file_hash.update(chunk)
    return file_hash.hexdigest()


def compute_relinks(hash_to_inos, ino_to_paths):
    for inos in hash_to_inos.values():
        if len(inos) == 1:
            continue
        best_ino = max(inos, key=lambda ino: len(ino_to_paths[ino]))
        best_path = elem(ino_to_paths[best_ino])
        for ino in inos - set([best_ino]):
            for file_path in ino_to_paths[ino]:
                yield (best_path, file_path)


if len(sys.argv) == 2 and sys.argv[1] == "--force":
    def relink(src, dst):
        os.unlink(dst)
        os.link(src, dst)
else:
    def relink(src, dst):
        print("Would point", dst, "at", src)


if __name__ == "__main__":
    main()
